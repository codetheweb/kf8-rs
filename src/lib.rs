use deku::prelude::*;
use nom::{
    branch::alt,
    bytes::complete::{tag, take, take_while},
    combinator::map,
    multi::count,
    number::complete::{be_u16, be_u24, be_u32, be_u8},
    sequence::tuple,
    IResult,
};
#[cfg(test)]
use proptest_derive::Arbitrary;
use std::{
    collections::HashMap,
    io::{BufRead, Read},
    iter::once,
    str::{self, FromStr},
};
use types::{BookHeader, Codepage, CompressionType, MobiHeader, MobiHeaderIdent, SectionHeader};

use crate::{
    constants::{MainLanguage, MetadataId, MetadataIdValue, SubLanguage},
    tag_map::{parse_tag_map, parse_tag_section},
};

#[macro_use]
extern crate lazy_static;

pub mod builder;
pub mod constants;
mod serialization;
mod tag_map;
pub mod types;
mod utils;

fn parse_compression_type(input: &[u8]) -> IResult<&[u8], CompressionType> {
    alt((
        map(tag([0x00, 0x01]), |_| CompressionType::None),
        map(tag([0x00, 0x02]), |_| CompressionType::PalmDoc),
        map(tag([0x44, 0x48]), |_| CompressionType::HuffCdic),
    ))(input)
}

fn parse_name(input: &[u8]) -> IResult<&[u8], String> {
    let (input, name_bytes) = take(32usize)(input)?;

    let name = map(take_while(|b| b != 0x00), |bytes| {
        // todo: should conditionally be utf8 or cp1252 based on codec in header
        str::from_utf8(bytes).unwrap_or_default().to_string()
    })(name_bytes)?;

    Ok((input, name.1))
}

fn parse_ident(input: &[u8]) -> IResult<&[u8], MobiHeaderIdent> {
    alt((tag("BOOKMOBI"), tag("TEXTREAD")))(input).map(|(input, ident)| {
        let ident = match ident {
            b"BOOKMOBI" => MobiHeaderIdent::BookMobi,
            b"TEXTREAD" => MobiHeaderIdent::TextRead,
            _ => unreachable!(),
        };
        (input, ident)
    })
}

fn parse_section_header(input: &[u8]) -> IResult<&[u8], SectionHeader> {
    let (input, (offset, flags, val)) = tuple((be_u32, be_u8, be_u24))(input)?;
    Ok((input, SectionHeader { offset, flags, val }))
}

fn parse_mobi_header(input: &[u8]) -> IResult<&[u8], MobiHeader> {
    let (input, header) = take(72usize)(input)?;
    let (input, _) = take(4usize)(input)?;
    let (input, num_sections) = be_u16(input)?;

    let (_, name) = parse_name(header)?;
    let (_, ident) = parse_ident(&header[0x3C..])?;

    let (mut input, mut section_headers) = (input, Vec::new());
    for _ in 0..num_sections {
        let (i, section_header) = parse_section_header(input)?;
        section_headers.push(section_header);
        input = i;
    }

    let (input, _) = tag([0x00, 0x00])(input)?;

    Ok((
        input,
        MobiHeader {
            name,
            num_sections,
            ident,
            section_headers,
        },
    ))
}

fn parse_codepage(input: &[u8]) -> IResult<&[u8], Codepage> {
    alt((
        map(tag([0x00, 0x00, 0x04, 0xe4]), |_| Codepage::Cp1252),
        map(tag([0x00, 0x00, 0xfd, 0xe9]), |_| Codepage::Utf8),
    ))(input)
}

// Some KF8 files have header length == 264 (generated by kindlegen 2.9?). See https://bugs.launchpad.net/bugs/1179144
// We choose 500 for future versions of kindlegen
const MAX_HEADER_LENGTH: usize = 500;

const NULL_INDEX: u32 = u32::MAX;

#[derive(Debug, PartialEq, Clone)]
#[cfg_attr(test, derive(Arbitrary))]
pub struct K8Header {
    skelidx: u32,
    fragidx: u32,
    guideidx: u32,
    fdst: u32,
    fdst_count: u32,
}

fn parse_book_header<'a>(data: &'a [u8]) -> IResult<&'a [u8], BookHeader> {
    let (_, header) =
        crate::serialization::MobiHeader::from_bytes((data, 0)).expect("could not parse header");

    Ok((
        data,
        BookHeader {
            compression_type: header.compression_type,
            records: header.last_text_record,
            records_size: header.text_record_size,
            encryption_type: header.encryption_type,
            doctype: types::DocType::Mobi,
            unique_id: header.uid,
            language: header.language_code.main,
            sub_language: header.language_code.sub,
            ncxidx: header.ncx_index,
            extra_flags: header.extra_data_flags,
            title: header.title,
            first_resource_section_index: header.first_resource_record as usize,
            kf8_metadata: Some(header.exth.metadata_value),
            standard_metadata: Some(header.exth.metadata_id),
            k8: Some(K8Header {
                skelidx: header.skel_index,
                fragidx: header.chunk_index,
                guideidx: header.guide_index,
                fdst: header.fdst_record,
                fdst_count: header.fdst_count,
            }),
        },
    ))
}

#[derive(Debug, PartialEq)]
pub struct MobiBookFragment {
    pub index: usize,
    pub content: Vec<u8>,
}

#[derive(Debug, PartialEq)]
pub struct MobiBookPart {
    pub filename: String,
    pub skeleton_head: Vec<u8>,
    pub fragments: Vec<MobiBookFragment>,
    pub skeleton_tail: Vec<u8>,
    pub start_offset: usize,
    pub end_offset: usize,
}

impl MobiBookPart {
    pub fn get_content(&self) -> Vec<u8> {
        let mut content = Vec::new();
        content.extend_from_slice(&self.skeleton_head);
        for fragment in &self.fragments {
            content.extend_from_slice(&fragment.content);
        }
        content.extend_from_slice(&self.skeleton_tail);
        content
    }
}

#[derive(Debug, PartialEq)]
pub enum ImageResourceKind {
    Cover,
    Thumbnail,
    Other,
}

#[derive(Debug, PartialEq)]
pub enum ResourceKind {
    Image(ImageResourceKind),
    Font,
    Stylesheet,
}

#[derive(Debug)]
pub struct Resource {
    pub kind: ResourceKind,
    pub data: Vec<u8>,
    pub file_type: infer::Type,
    pub flow_index: Option<usize>,
}

#[derive(Debug)]
pub struct MobiBook {
    mobi_header: MobiHeader,
    pub book_header: BookHeader,
    pub fragment_table: Vec<FragmentTableEntry>,
    content: String,
    pub parts: Vec<MobiBookPart>,
    pub resources: Vec<Resource>,
}

fn get_section_data<'a>(data: &'a [u8], mobi_header: &MobiHeader, section_i: usize) -> &'a [u8] {
    let end_offset = if section_i == (mobi_header.num_sections - 1).into() {
        data.len()
    } else {
        mobi_header.section_headers[section_i + 1].offset as usize
    };

    let section_header = &mobi_header.section_headers[section_i];

    &data[section_header.offset as usize..end_offset]
}

pub fn parse_book(input: &[u8]) -> IResult<&[u8], MobiBook> {
    let original_input = input;
    let original_input_length = input.len();
    let (input, mobi_header) = parse_mobi_header(input)?;

    let (input, _) = take(2usize)(input)?; // Skip 2 bytes

    // todo: use first section offset instead of manually skipping bytes above?
    let (_, book_header) = parse_book_header(
        &original_input[mobi_header.section_headers.first().unwrap().offset as usize..],
    )?;

    if book_header.k8.is_none() {
        panic!("book does not have a k8 header, either malformed or unsupported")
    }

    let mut raw_ml = Vec::new();
    for (i, section_header) in mobi_header.section_headers.iter().enumerate().skip(1) {
        if i > book_header.records as usize {
            break;
        }

        let section_data = get_section_data(original_input, &mobi_header, i);
        let section_data = &section_data
            [..section_data.len() - book_header.sizeof_trailing_section_entries(section_data)];

        let decompressed = palmdoc_compression::calibre::decompress(section_data);

        raw_ml.extend_from_slice(&decompressed);
    }

    // Parse flow boundaries
    let fdst_section_data = get_section_data(
        original_input,
        &mobi_header,
        book_header.k8.clone().unwrap().fdst as usize,
    );

    let (_, fdst_table) = parse_fdst(fdst_section_data, raw_ml.len()).unwrap();

    let mut flows = Vec::new();

    for (starts_at, ends_at) in fdst_table.iter().zip(fdst_table.iter().skip(1)) {
        let flow = &raw_ml[*starts_at..*ends_at];
        flows.push(flow);
    }

    let text = *flows.first().unwrap();

    let (_, skeleton_table) = parse_index_data(
        original_input,
        &mobi_header,
        book_header.k8.clone().unwrap().skelidx as usize,
    )?;

    let (_, fragment_table) = parse_index_data(
        original_input,
        &mobi_header,
        book_header.k8.clone().unwrap().fragidx as usize,
    )?;

    let fragment_table = index_table_to_fragment_table(&fragment_table);

    let mut parts = vec![];

    let mut fragment_i = 0;
    for (i, skeleton_entry) in index_table_to_skeleton_table(&skeleton_table)
        .iter()
        .enumerate()
    {
        let mut base_ptr = skeleton_entry.start_offset + skeleton_entry.len;

        let mut fragments: Vec<MobiBookFragment> = vec![];

        let first_fragment = fragment_table.get(fragment_i).unwrap();
        let split_skeleton_at = first_fragment.insert_position as usize;

        // todo: zip?
        let mut filename: String = "".to_string();
        for i in 0..skeleton_entry.fragment_table_record_count {
            let fragment_entry = fragment_table.get(fragment_i).unwrap();

            if i == 0 {
                filename = format!("part{}.xhtml", fragment_entry.file_number);
            }

            let fragment_text = &text[base_ptr..base_ptr + fragment_entry.len as usize];

            fragments.push(MobiBookFragment {
                index: fragment_i,
                content: fragment_text.to_vec(),
            });

            base_ptr += fragment_entry.len as usize;
            fragment_i += 1;
        }

        let skeleton_head = &text[skeleton_entry.start_offset..split_skeleton_at];
        let skeleton_tail =
            &text[split_skeleton_at..skeleton_entry.start_offset + skeleton_entry.len];

        parts.push(MobiBookPart {
            filename,
            skeleton_head: skeleton_head.to_vec(),
            fragments,
            skeleton_tail: skeleton_tail.to_vec(),
            start_offset: skeleton_entry.start_offset,
            end_offset: base_ptr,
        });
    }

    // Resources
    let mut resources: Vec<Resource> = vec![];

    // todo: handle SVGs/images, CDATA?
    let stylesheets = flows.iter().skip(1);

    let mut info = infer::Infer::new();
    info.add("text/css", "css", |_| true);

    for (i, stylesheet) in stylesheets.enumerate() {
        resources.push(Resource {
            kind: ResourceKind::Stylesheet,
            data: stylesheet.to_vec(),
            file_type: info.get(stylesheet).unwrap(),
            flow_index: Some(i + 1),
        });
    }

    let cover_offset = book_header.first_resource_section_index
        + *book_header
            .kf8_metadata
            .as_ref()
            .unwrap()
            .get(&MetadataIdValue::CoverOffset)
            .unwrap()
            .first()
            .unwrap() as usize;

    let thumbnail_offset = book_header.first_resource_section_index
        + *book_header
            .kf8_metadata
            .as_ref()
            .unwrap()
            .get(&MetadataIdValue::ThumbOffset)
            .unwrap()
            .first()
            .unwrap() as usize;

    for section_i in book_header.first_resource_section_index..mobi_header.num_sections as usize {
        let data = get_section_data(original_input, &mobi_header, section_i);
        let (input, resource_type) = take(4usize)(data)?;

        match resource_type {
            b"FLIS" | b"FCIS" | b"FDST" | b"DATP" => {
                // todo?
            }
            b"SRCS" => {
                // todo
            }
            b"PAGE" => {
                // todo
            }
            b"CMET" => {
                // todo
            }
            b"FONT" => {
                // todo
            }
            b"CRES" => {
                // todo
            }
            b"CONT" => {
                // todo
            }
            b"kind" => {
                // todo
            }
            [0xa0, 0xa0, 0xa0, 0xa0] => {
                // todo
                println!("byte pattern, empty image?")
            }
            b"RESC" => {
                // todo
            }
            // EOF
            [0xe9, 0x8e, 0x0d, 0x0a] => {
                // todo
            }
            b"BOUN" => {
                // todo
            }
            _ => {
                // Should be an image
                let file_type = infer::get(data);

                if section_i == cover_offset {
                    resources.push(Resource {
                        kind: ResourceKind::Image(ImageResourceKind::Cover),
                        data: data.to_vec(),
                        file_type: file_type.unwrap(),
                        flow_index: None,
                    })
                } else if section_i == thumbnail_offset {
                    resources.push(Resource {
                        kind: ResourceKind::Image(ImageResourceKind::Thumbnail),
                        data: data.to_vec(),
                        file_type: file_type.unwrap(),
                        flow_index: None,
                    })
                } else {
                    resources.push(Resource {
                        kind: ResourceKind::Image(ImageResourceKind::Other),
                        data: data.to_vec(),
                        file_type: file_type.unwrap(),
                        flow_index: None,
                    })
                }
            }
        }
    }

    Ok((
        input,
        MobiBook {
            mobi_header,
            book_header,
            fragment_table,
            // todo: this should not be lossy
            content: String::from_utf8_lossy(&raw_ml).to_string(),
            parts,
            resources,
        },
    ))
}

#[derive(Debug)]
struct IndexTableEntry {
    file_number: usize,
    label: String,
    tag_map: HashMap<u8, Vec<u32>>,
}

fn parse_index_data<'a>(
    original_input: &'a [u8],
    mobi_header: &MobiHeader,
    section_i: usize,
) -> IResult<&'a [u8], Vec<IndexTableEntry>> {
    // Parse INDX header
    let indx_section_data = get_section_data(original_input, mobi_header, section_i);
    let (_, indx_header) = parse_indx_header(indx_section_data).unwrap();

    let (_, tag_section) =
        parse_tag_section(&indx_section_data[indx_header.len as usize..]).unwrap();

    let mut skeleton_table = vec![];

    for i in (section_i + 1)..(section_i + 1 + indx_header.count as usize) {
        let data = get_section_data(original_input, mobi_header, i);
        let (_, header) = parse_indx_header(data).unwrap();

        let (_, indx_offsets) =
            count(be_u16, header.count as usize)(&data[header.start as usize + 4..])?;

        for (i, beginning_offset) in indx_offsets.iter().enumerate() {
            let (remaining, segment) =
                parse_indx_text_segment(&data[*beginning_offset as usize..])?;

            let (_, tag_map) = parse_tag_map(
                tag_section.control_byte_count,
                &tag_section.table,
                remaining,
            )
            .unwrap();

            skeleton_table.push(IndexTableEntry {
                file_number: i,
                label: segment,
                tag_map,
            });
        }
    }

    Ok((&[], skeleton_table))
}

#[derive(Debug)]
struct SkeletonTableEntry {
    file_number: usize,
    label: String,
    fragment_table_record_count: usize,
    start_offset: usize,
    len: usize,
}

fn index_table_to_skeleton_table(table_entries: &[IndexTableEntry]) -> Vec<SkeletonTableEntry> {
    table_entries
        .iter()
        .map(|entry| SkeletonTableEntry {
            file_number: entry.file_number,
            label: entry.label.clone(),
            fragment_table_record_count: entry.tag_map.get(&1).unwrap()[0] as usize,
            start_offset: entry.tag_map.get(&6).unwrap()[0] as usize,
            len: entry.tag_map.get(&6).unwrap()[1] as usize,
        })
        .collect()
}

#[derive(Debug)]
pub struct FragmentTableEntry {
    pub insert_position: u32,
    id_text: String,
    file_number: u32,
    seq_number: u32,
    start_pos: u32,
    len: u32,
}

fn index_table_to_fragment_table(table_entries: &[IndexTableEntry]) -> Vec<FragmentTableEntry> {
    table_entries
        .iter()
        .map(|entry| FragmentTableEntry {
            insert_position: u32::from_str(&entry.label).unwrap(),
            // todo
            id_text: "".to_string(),
            file_number: entry.tag_map.get(&3).unwrap()[0],
            seq_number: entry.tag_map.get(&4).unwrap()[0],
            start_pos: entry.tag_map.get(&6).unwrap()[0],
            len: entry.tag_map.get(&6).unwrap()[1],
        })
        .collect()
}

fn parse_indx_text_segment(input: &[u8]) -> IResult<&[u8], String> {
    let (input, len) = be_u8(input)?;
    let (input, segment) = take(len as usize)(input)?;

    // todo: remove unwrap
    Ok((input, String::from_utf8(segment.to_vec()).unwrap()))
}

fn parse_fdst(input: &[u8], raw_ml_len: usize) -> IResult<&[u8], Vec<usize>> {
    let (input, _) = tag(b"FDST")(input)?;
    let (input, _) = take(4usize)(input)?;
    let (input, num_sections) = be_u32(input)?;
    let (input, sections) = count(be_u32, num_sections as usize * 2)(input)?;

    let positions = sections
        .iter()
        .step_by(2)
        .map(|x| *x as usize)
        .chain(once(raw_ml_len))
        .collect();

    Ok((input, positions))
}

#[derive(Debug)]
struct INDXHeader {
    len: u32,
    nul1: u32,
    type_field: u32,
    gen: u32,
    start: u32,
    count: u32,
    code: u32,
    lng: u32,
    total: u32,
    ordt: u32,
    ligt: u32,
    nligt: u32,
    nctoc: u32,
    ordt1: Option<Vec<u8>>,
    ordt2: Option<Vec<u16>>,
}

fn parse_indx_header(input: &[u8]) -> IResult<&[u8], INDXHeader> {
    let (input, _) = tag(b"INDX")(input)?;

    let (
        input,
        (len, nul1, type_field, gen, start, count, code, lng, total, ordt, ligt, nligt, nctoc),
    ) = tuple((
        be_u32, be_u32, be_u32, be_u32, be_u32, be_u32, be_u32, be_u32, be_u32, be_u32, be_u32,
        be_u32, be_u32,
    ))(input)?;

    let (input, (ocnt, oentries, op1, op2, _otagx)) =
        tuple((be_u32, be_u32, be_u32, be_u32, be_u32))(input)?;

    let ordt1;
    let ordt2;

    if code == 0xfdea || ocnt != 0 || oentries > 0 {
        ordt1 = Some(take(oentries as usize)(input)?.1.to_vec());
        ordt2 = Some(nom::multi::count(be_u16, oentries as usize)(input)?.1);
    } else {
        ordt1 = None;
        ordt2 = None;
    }

    Ok((
        input,
        INDXHeader {
            len,
            nul1,
            type_field,
            gen,
            start,
            count,
            code,
            lng,
            total,
            ordt,
            ligt,
            nligt,
            nctoc,
            ordt1,
            ordt2,
        },
    ))
}

#[cfg(test)]
mod tests {
    use builder::{write_book_header, write_language, write_palmdb_header};
    use cookie_factory::gen;
    use proptest::{arbitrary::any, proptest};
    use types::mobi_header;

    use super::*;

    #[test]
    fn extract_raw_html() {
        // todo
        env_logger::init();
        let mut reader = std::fs::File::open("resources/war_and_peace.azw3").unwrap();
        let mut data = Vec::new();
        reader.read_to_end(&mut data).unwrap();

        let (_, book) = parse_book(&data).unwrap();

        let mut expected_html_reader =
            std::fs::File::open("resources/war_and_peace.rawml").unwrap();
        let mut expected_html = String::new();
        expected_html_reader
            .read_to_string(&mut expected_html)
            .unwrap();

        assert_eq!(book.content, expected_html);
    }

    proptest! {
        #[test]
        fn roundtrip_mobi_header(header in mobi_header()) {
            let serialized = Vec::new();
            let (serialized, _) = gen(write_palmdb_header(&header), serialized).expect("could not serialize");
            let (leftover, parsed) = parse_mobi_header(&serialized).expect("could not parse");

            assert_eq!(header, parsed);
            assert_eq!(0, leftover.len());
        }

        #[test]
        fn roundtrip_book_header(header in any::<BookHeader>()) {
            let serialized = Vec::new();
            let (serialized, _) = gen(write_book_header(&header), serialized).expect("could not serialize");
            let (leftover, parsed) = parse_book_header(&serialized).expect("could not parse");

            assert_eq!(header, parsed);
            // todo
            // assert_eq!(0, leftover.len());
        }
    }
}
